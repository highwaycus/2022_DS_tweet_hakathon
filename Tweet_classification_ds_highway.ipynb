{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d7d949",
   "metadata": {},
   "source": [
    "### ReadMe\n",
    "\n",
    "To execute the model, you have to run *main()*. In the following, *main()* is at the end of document.\n",
    "##### Video\n",
    "The link of video:https://drive.google.com/file/d/1DyASFI9fA9KTf-JdRDTkVtSekQ_mwSw1/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337dd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Basic Settings\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from keras.preprocessing import sequence\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "def display_set():\n",
    "    np.set_printoptions(precision=5, suppress=True, linewidth=150)\n",
    "    pd.set_option('display.width', 10000)\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "    pd.set_option('display.max_rows', 2000)\n",
    "    pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22568844",
   "metadata": {},
   "source": [
    "### NLP Training\n",
    "I take out the feature *full_text* and do NLP training. In train data, I collect the most frequent words, and use LSTM, RNN and GRU to train the data. Finally I use only SimpleRNN. The output is the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddac0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_process(train, test, max_words=800, maxlen=100, test_amplify=1):\n",
    "    train = train.fillna(\" \")\n",
    "    test = test.fillna(\" \")\n",
    "    print(np.sum(np.array(train.isnull() == True), axis=0))\n",
    "    print('train[\\'Target\\'].unique()=', train['Target'].unique())\n",
    "    X_train = train['ID'] + ' ' + train['full_text']\n",
    "    y_train = train['Target']\n",
    "    X_test = test['ID'] + ' ' + test['full_text']\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "    # ONLY GIVE MOST FREQUENT WORDS ids\n",
    "    if test_amplify > 1:\n",
    "        X_test = X_test.append(X_test.sample(int(len(X_test) *(test_amplify - 1)), replace=True))\n",
    "    elif test_amplify < 1:\n",
    "        X_test =  X_test.sample(int(len(X_test) * test_amplify))\n",
    "    tokenizer.fit_on_texts(list(X_train) + list(X_test))  # tokenizer training\n",
    "    X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "    # pad ids series, make them the same length\n",
    "    X_train_tokens_pad = sequence.pad_sequences(X_train_tokens, maxlen=maxlen, padding='post')\n",
    "    return X_train_tokens_pad, y_train, tokenizer\n",
    "\n",
    "\n",
    "def model_create(max_words, maxlen, units=64, embeddings_dim=50, clf_name='LSTM'):\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras.layers import Embedding, LSTM, GRU, SimpleRNN, Dense\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words,  # Size of the vocabulary\n",
    "                        output_dim=embeddings_dim,  # word-embedded dimensional\n",
    "                        input_length=maxlen))\n",
    "    if clf_name == 'LSTM':\n",
    "        model.add(LSTM(units=units))\n",
    "    elif clf_name == 'GRU':\n",
    "        model.add(GRU(units=units))\n",
    "    elif clf_name == 'SimpleRNN':\n",
    "        model.add(SimpleRNN(units=units))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, X_train_tokens_pad, y_train, embeddings_dim, max_words, clf_name='', plotting=True):\n",
    "    # Training\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy']) \n",
    "    history = model.fit(X_train_tokens_pad, y_train,\n",
    "                        batch_size=128, epochs=10, validation_split=0.2)\n",
    "    model.save(\"full_text_cat_lstm_mw{}-dim{}.h5\".format(max_words, embeddings_dim))  # save models\n",
    "\n",
    "    # plot\n",
    "    if plotting:\n",
    "        from matplotlib import pyplot as plt\n",
    "        pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "        plt.grid(True)\n",
    "        plt.title(clf_name)\n",
    "        plt.show()\n",
    "    return model\n",
    "\n",
    "\n",
    "def one_model_train_process(stack_predict, X_train_tokens_pad, test, tokenizer, y_train, max_words, maxlen, embeddings_dim, clf, test_amplify, plotting=False):\n",
    "    print('\\n##################################')\n",
    "    print('{}:'.format(clf))\n",
    "    model1 = model_create(max_words, maxlen, units=64, embeddings_dim=embeddings_dim, clf_name=clf)\n",
    "    model1 = train_model(model1, X_train_tokens_pad, y_train, embeddings_dim, max_words, plotting=plotting)\n",
    "    test_tokens_pad = test_data_process(test, tokenizer, maxlen=maxlen)\n",
    "    current_time = datetime.datetime.today().strftime('%Y%m%d_%H%M')\n",
    "    file_name = 'tweet-use_-amplify{}-maxlen{}-clf-{}-{}'.format(test_amplify, maxlen, clf, current_time)\n",
    "    test_df, pred_prob = test_result(model=model1, X_test_tokens_pad=test_tokens_pad, save_path=route,\n",
    "                                     file_name=file_name)\n",
    "    stack_predict[clf] = {'label': test_df, 'prob': pred_prob}\n",
    "    del model1\n",
    "    gc.collect()\n",
    "    return stack_predict\n",
    "\n",
    "\n",
    "def stacking(maxlen, embeddings_dim, max_words, route='2022_tweet/new/'):\n",
    "    train_data = pd.read_csv(route + 'train_dataset.csv')\n",
    "\n",
    "    train_data = train_data.fillna(' ')\n",
    "    train = train_data[['ID', 'full_text', 'Target']]\n",
    "    train['ID'] = train['ID'].apply(lambda x: str(x))\n",
    "    train['full_text'] = train['full_text'].apply(lambda x: str(x))\n",
    "    test_amplify = 5\n",
    "    # train = train.sample(frac=1, random_state=404, replace=False)\n",
    "    train = train.sample(frac=1, replace=False)\n",
    "    train = train.reset_index(drop=True)\n",
    "    train1 = train.loc[:2560]\n",
    "    test = train.loc[2560:]\n",
    "    test = test[['ID', 'full_text']]\n",
    "    test_label = train.loc[2560:, ['ID', 'Target']]\n",
    "    X_train_tokens_pad, y_train, tokenizer = train_data_process(train1, test=test, max_words=max_words,\n",
    "                                                                maxlen=maxlen, test_amplify=test_amplify)\n",
    "    stack_predict = {}\n",
    "    # for clf in ['LSTM', 'GRU', 'SimpleRNN']:\n",
    "    for clf in ['SimpleRNN']:\n",
    "        stack_predict = one_model_train_process(stack_predict, X_train_tokens_pad, test, tokenizer, y_train, max_words, maxlen, embeddings_dim, clf, test_amplify)\n",
    "    stack_predict_vote = []\n",
    "    for i in range(len(test)):\n",
    "        vote_ratio = np.mean([stack_predict[ml]['prob'][i] for ml in stack_predict])\n",
    "        if vote_ratio > 0.5:\n",
    "            stack_predict_vote.append(1)\n",
    "        else:\n",
    "            stack_predict_vote.append(0)\n",
    "    stack_df = pd.DataFrame({'ID': [j for j in range(1, len(test) + 1)], 'Target': stack_predict_vote})\n",
    "    acc_value = test_accuracy(stack_df, test_label)\n",
    "    record_model_result(None, acc_value,\n",
    "                        model_name='stacking-tuning-maxlen{}-maxw{}-dim{}'.format(maxlen, max_words, embeddings_dim),\n",
    "                        model_path=route)\n",
    "    del stack_df, X_train_tokens_pad, y_train, tokenizer\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def stacking_try_para_main():\n",
    "    for maxlen in [50, 100, 150, 200, 250, 300]:\n",
    "        for embeddings_dim in [40, 50, 60, 70, 80]:\n",
    "            for max_words in [300, 400, 500, 600, 700, 800, 900]:\n",
    "                print('\\n=================================================')\n",
    "                print('maxlen=', maxlen, 'embeddings dim=', embeddings_dim, 'max words=', max_words)\n",
    "                stacking(maxlen, embeddings_dim, max_words)\n",
    "\n",
    "\n",
    "def nlp_feature(train_data, test_data, route):\n",
    "    train = train_data[['ID', 'full_text', 'Target']]\n",
    "    test = test_data[['ID', 'full_text']]\n",
    "    print(train['Target'].unique())\n",
    "    train['ID'] = train['ID'].apply(lambda x: str(x))\n",
    "    train['full_text'] = train['full_text'].apply(lambda x: str(x))\n",
    "    test['ID'] = test['ID'].apply(lambda x: str(x))\n",
    "    test['full_text'] = test['full_text'].apply(lambda x: str(x))\n",
    "    maxlen = 100\n",
    "    max_words = 500\n",
    "    embeddings_dim = 50\n",
    "    test_amplify = 5\n",
    "    train = train.sample(frac=1, random_state=404, replace=False)\n",
    "    train = train.reset_index(drop=True)\n",
    "    X_train_tokens_pad, y_train, tokenizer = train_data_process(train, test=test, max_words=max_words,\n",
    "                                                                maxlen=maxlen, test_amplify=test_amplify)\n",
    "    stack_predict = {}\n",
    "    # for clf in ['LSTM', 'GRU', 'SimpleRNN']:\n",
    "    for clf in ['SimpleRNN']:\n",
    "        stack_predict = one_model_train_process(stack_predict, X_train_tokens_pad, test, tokenizer, y_train, max_words,\n",
    "                                                maxlen, embeddings_dim, clf, test_amplify)\n",
    "    # stack_predict_vote = []\n",
    "    # for i in range(len(test)):\n",
    "    #     vote_ratio = np.mean([stack_predict[ml]['prob'][i] for ml in stack_predict])\n",
    "    #     if vote_ratio > 0.5:\n",
    "    #         stack_predict_vote.append(1)\n",
    "    #     else:\n",
    "    #         stack_predict_vote.append(0)\n",
    "    # stack_df = pd.DataFrame({'ID': [j for j in range(1, len(test) + 1)], 'Target': stack_predict_vote})\n",
    "    stack_df = pd.DataFrame({'ID': [j for j in range(1, len(test) + 1)], 'Target': stack_predict['SimpleRNN']['prob']})\n",
    "    # acc_value = test_accuracy(stack_df, test_label)\n",
    "    current_time = datetime.datetime.today().strftime('%Y%m%d_%H%M')\n",
    "    stack_df.to_csv(\n",
    "        route + 'submit_stacking-use_tweet_maxlen{}-maxw{}-dim{}-{}.csv'.format(maxlen, max_words, embeddings_dim,\n",
    "                                                                         current_time), index=None, line_terminator=\"\\n\")\n",
    "    return stack_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b78c0",
   "metadata": {},
   "source": [
    "### Model: second part\n",
    "After I got nlp features, I regard it as a new feature. First I do feature engineering, that is, I turn string features to integer values. Also I look for sepecail words in the feature *Additional.Comments* and make dummy features.\n",
    "\n",
    "When training, I applied random forest classifier. I grouped the datasets by the *sample_name*. This is because I assumed the samples in same group (same *sample_name*) would have similar characteristics than between different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train_shift1):\n",
    "    for col in ['Still.Exists.x', 'Still.Exists.y', 'In.English.x', 'In.English.y', 'Sarcasm.x', 'Sarcasm.y']:\n",
    "        train_shift1[col] = train_shift1[col].apply(lambda x: int(x))\n",
    "    train_shift1['Additional.Comments.x_jews'] = train_shift1['Additional.Comments.x'].apply(\n",
    "        lambda x: 1 if (('Jews' in x) or ('jews' in x)) else 0)\n",
    "    train_shift1['Additional.Comments.x_israel'] = train_shift1['Additional.Comments.x'].apply(\n",
    "        lambda x: 1 if (('israel' in x) or ('Israel' in x)) else 0)\n",
    "    train_shift1['Additional.Comments.y_jews'] = train_shift1['Additional.Comments.y'].apply(\n",
    "        lambda x: 1 if (('Jews' in x) or ('jews' in x)) else 0)\n",
    "    train_shift1['Additional.Comments.y_israel'] = train_shift1['Additional.Comments.y'].apply(\n",
    "        lambda x: 1 if (('israel' in x) or ('Israel' in x)) else 0)\n",
    "    x_list = ['Still.Exists.x', 'Still.Exists.y', 'In.English.x', 'In.English.y',\n",
    "              'Sarcasm.x', 'Sarcasm.y', 'Additional.Comments.x_jews', 'Additional.Comments.y_jews',\n",
    "              'Additional.Comments.x_israel', 'Additional.Comments.y_israel',\n",
    "              'Sentiment.Rating.x', 'Sentiment.Rating.y', 'Calling.Out.x', 'Calling.Out.y', 'Is.About.the.Holocaust.x',\n",
    "              'Is.About.the.Holocaust.y', 'Is.About.The.Holocaust.x', 'Is.About.The.Holocaust.y', 'nlp_feature']\n",
    "    train_X = train_shift1[x_list]\n",
    "    for col in train_X.columns:\n",
    "        train_X[col] = train_X[col].apply(lambda x: np.NAN if x == ' ' else x)\n",
    "    train_X = train_X.fillna(0.5)\n",
    "    return train_X\n",
    "\n",
    "\n",
    "def group_training(train_data, train_shift):\n",
    "    clf_record = {sn: np.NAN for sn in train_data['sample_name'].value_counts().keys()}\n",
    "    for sn in list(train_data['sample_name'].value_counts().keys()):\n",
    "        train_sample = train_shift[train_shift['sample_name'] == sn]\n",
    "        train_X = feature_engineering(train_sample)\n",
    "        train_y = train_sample['Target']\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        clf = RandomForestClassifier(n_estimators=20, max_depth=3, random_state=703)\n",
    "        clf.fit(train_X, train_y)\n",
    "        print(sn, clf.score(train_X, train_y))\n",
    "        clf_record[sn] = clf\n",
    "        del clf\n",
    "        gc.collect()\n",
    "    return clf_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec358a",
   "metadata": {},
   "source": [
    "### Evaluation Functions\n",
    "These are functions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_process(data, tokenizer, maxlen=100):\n",
    "    data = data.fillna(\" \")\n",
    "    X_data = data['ID'] + ' ' + data['full_text']\n",
    "    X_data_tokens = tokenizer.texts_to_sequences(X_data)\n",
    "    X_data_tokens_pad = sequence.pad_sequences(X_data_tokens, maxlen=maxlen, padding='post')\n",
    "    return X_data_tokens_pad\n",
    "\n",
    "\n",
    "def test_result(model, X_test_tokens_pad, save_path, file_name):\n",
    "    pred_prob = model.predict(X_test_tokens_pad).squeeze()\n",
    "    pred_class = np.asarray(pred_prob > 0.5).astype(np.int32)\n",
    "    output = pd.DataFrame({'Id': [i for i in range(1, len(pred_class) + 1)], 'Flag': pred_class})\n",
    "    output.to_csv('{}{}.csv'.format(save_path, file_name), index=None, line_terminator=\"\\n\")\n",
    "    return output, pred_prob\n",
    "\n",
    "\n",
    "def test_accuracy(test_result, test_label):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    y_pred = test_result['Target'].tolist()\n",
    "    y_true = test_label['Target'].tolist()\n",
    "    accuracy_value = accuracy_score(y_true, y_pred)\n",
    "    print('accuracy = ', accuracy_value)\n",
    "    return accuracy_value\n",
    "\n",
    "\n",
    "def record_model_result(model, acc_value, model_name, model_path=''):\n",
    "    if not os.path.isfile(model_path + 'model_prediction_record.npy'):\n",
    "        record = {}\n",
    "    else:\n",
    "        record = np.load(model_path + 'model_prediction_record.npy', allow_pickle=True).item()\n",
    "    record[model_name] = {'acc': acc_value}\n",
    "    np.save(model_path + 'model_prediction_record.npy', record, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_accuracy(test_result, test_label):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    y_pred = test_result['Target'].tolist()\n",
    "    y_true = test_label['Target'].tolist()\n",
    "    accuracy_value = accuracy_score(y_true, y_pred)\n",
    "    print('accuracy = ', accuracy_value)\n",
    "    return accuracy_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021bc392",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "When prediction, we use models according to samlples' *sample_name* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    display_set()\n",
    "    route = '2022_tweet/new/'\n",
    "    train_data = pd.read_csv(route + 'train_dataset.csv')\n",
    "    test_data = pd.read_csv(route + 'test_dataset.csv')\n",
    "    train_data = train_data.dropna(subset=['Target'])\n",
    "    \n",
    "    ####################################\n",
    "    # NLP feature parameter tuning\n",
    "    ###################################\n",
    "    stacking_try_para_main()\n",
    "    record = np.load(route + 'model_prediction_record.npy', allow_pickle=True).item()\n",
    "    print(sorted(record.items(), key=lambda x: x[1]['acc'], reverse=True)[0])\n",
    "    maxlen = 50\n",
    "    max_words = 500\n",
    "    embeddings_dim = 70\n",
    "    \n",
    "    ####################################\n",
    "    # NLP Feature Create\n",
    "    ####################################\n",
    "    train_data = train_data.fillna(' ')\n",
    "    test_data = test_data.fillna(' ')\n",
    "    nlp_feature_data = nlp_feature(train_data, train_data, route)\n",
    "    train_shift = train_data.sample(frac=1, random_state=404, replace=False)\n",
    "    train_shift['nlp_feature'] = nlp_feature_data['Target'] # Probability\n",
    "    \n",
    "    #####################################\n",
    "    # Model second part\n",
    "    #####################################\n",
    "    clf_record = group_training(train_data, train_shift)\n",
    "    \n",
    "    ######################################\n",
    "    # Prediction\n",
    "    ######################################\n",
    "    test_nlp_feature_data = nlp_feature(train_data, test_data, route)\n",
    "    test_data['nlp_feature'] = test_nlp_feature_data['Target']\n",
    "    test_pred = {}\n",
    "    for sn in list(train_data['sample_name'].value_counts().keys()):\n",
    "        test_sample = test_data[test_data['sample_name'] == sn]\n",
    "        test_X = feature_engineering(test_sample)\n",
    "        clf1 = clf_record[sn]\n",
    "        pred_Y = clf1.predict(test_X)\n",
    "        for i in range(len(test_sample)):\n",
    "            test_pred[test_sample['ID'].tolist()[i]] = pred_Y[i]\n",
    "\n",
    "    stack_df = pd.DataFrame({'ID': test_data['ID'], 'Target': np.NAN})\n",
    "    for i in stack_df.index:\n",
    "        stack_df.loc[i, 'Target'] = test_pred[stack_df.loc[i, 'ID']]\n",
    "    stack_df['ID'] = [i for i in range(1, len(stack_df) + 1)]\n",
    "    current_time = datetime.datetime.today().strftime('%Y%m%d_%H%M')\n",
    "    stack_df.to_csv(\n",
    "        route + 'submit-sn_rf_use_tweet_maxlen{}-maxw{}-dim{}-{}.csv'.format(maxlen, max_words, embeddings_dim,\n",
    "                                                                             current_time), index=None,\n",
    "        line_terminator=\"\\n\")\n",
    "\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
